

# Data Science pathway

The insights you get from data science can feel like a gift to your business, but you don't get to just open your hands and get it delivered to you with a bow on it. Really, there are a lot of moving parts and things that have to be planned and coordinated for all of this to work properly. I like to think of data science projects like walking down a pathway, where each step gets you closer to the goal that you have in mind. And with that I want to introduce you to a way of thinking about the data science pathway. It begins with planning your project. You first need to define your goals. What is it that you're actually trying to find out or accomplish? That way you can know when you're on target or when you need to redirect a little bit. You need to organize your resources. That can include things as simple as getting the right computers and the software, accessing the data, getting people and their time available. You need to coordinate the work of those people because data science is a team effort. Not everybody's going to be doing the same thing and some things have to happen first and some happen later. You also need to schedule the project so it doesn't expand to fill up an enormous amount of time. Time boxing, or saying we will accomplish this task in this amount of time, can be especially useful in working on a tight timeframe or you have a budget and you're working with a client. After planning, the next step is going to be wrangling, or preparing the data. That means you need to first get the data. You may be gathering new data, you may be using open data sources, you may be using public APIs, but you have to actually get the raw materials together. The next one, step six, is cleaning the data, which actually is an enormous task within data science. It's about getting the data ready so it fits into the paradigm, for instance, the program and the applications that you're using, that you can process it to get the insight that you need. Once the data's prepared and it's in your computer, you need to explore the data, maybe making visualizations, maybe doing some numerical summaries, a way of getting a feel of what's going on in there. And then, based on your exploration, you may need to refine the data. You may need to re-categorize cases. You may need to combine variables into new scores. Any of the things that can help you get it prepared for the insight. The third category in your data pathway is modeling. This is where you actually create the statistical model and you do the linear regression. You do the decision tree. You do the deep learning neural network. But then, you need to validate the model. How well do you know this is going to generalize from the current data set to other data sets. In a lot of research that step is left out and you often end up with conclusions that fall apart when you go to new places. So, validation's a very important part of this. The next step is evaluating the model. How well does it fit the data? What's the return on investment for it? How usable is it going to be? And then, based on those, you may need to refine the model. You may need to try processing a different way, adjust the parameters in your neural network, get additional variables to include in your linear regression. Any one of those can help you build a better model to achieve the goals that you had in mind in the first place. And then finally, the last part of the data pathway is applying the model and that includes presenting the model, showing what you learned to other people, to the decision makers, to the invested parties, to your client, so they know what it is that you've found. Then you deploy the model. Say for instance, you created a recommendation engine. You actually need to put it online so that it can start providing these recommendations to clients or you put it into a dashboard so it can start providing recommendations to your decision makers. You will eventually need to revisit the model, see how well it's performing, especially when you have new data and maybe a new context in which it's operating. And then, you may need to revise it and try the process over again. And then finally, once you've done all of this there's the matter of archiving the assets, really cleaning up after yourself is very important in data science. It includes documenting where the data came from and how you process it. It includes commenting the code that you used to analyze it. It includes making things future proof. All of these together can make the project more successful, easier to manage, easier to get the return on investment calculations for it, and those together will make the project more successful by following each of these steps. Taken together those steps on the pathway get you to your goal. It could be an amazing view at the end of your hike, or it could be an amazing insight into your business model, which was your purpose all along.



## Planning

1. Define goals
2. Organize resources
3. Coordinate people
4. Schedule project



## Wrangling

5. Get data
6. Clean data
7. Explore data
8. Refine data



## Modeling

9. Create model
10. Validate model
11. Evaluate model
12. Refine model



## Applying

13. Present model
14. Deploy model
15. Revisit model
16. Archive assets







# References

* Linkedin - Data Science Foundations: Fundamentals

  * www.linkedin.com/learning/data-science-foundations-fundamentals-6

    